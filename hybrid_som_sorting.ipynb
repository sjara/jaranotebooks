{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOM to reduce numSpikes, then t-SNE to embed and HDBSCAN to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/pymodules/python2.7/matplotlib/__init__.py:1173: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time as time\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from jaratoolbox import loadopenephys\n",
    "import sys\n",
    "import SOMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a giant dataset, 1mil spikes\n",
    "\n",
    "animalName='adap020'\n",
    "ephysLoc = '/home/nick/data/ephys/'\n",
    "ephysPath = os.path.join(ephysLoc, animalName)\n",
    "ephysFn='2016-05-25_16-33-09'\n",
    "tetrode=2\n",
    "spikesFn = os.path.join(ephysPath, ephysFn, 'Tetrode{}.spikes'.format(tetrode))\n",
    "dataSpikes = loadopenephys.DataSpikes(spikesFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117928"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataSpikes.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(numSpikes, numChans, numSamples) = np.shape(dataSpikes.samples)\n",
    "\n",
    "allWaves = dataSpikes.samples.reshape(numSpikes, numChans*numSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(SOMPY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Training...\n",
      " pca_linear_initialization took: 8.410000 seconds\n",
      " Rough training...\n",
      " radius_ini: 4.000000 , radius_final: 1.000000, trainlen: 1\n",
      "\n",
      " epoch: 1 ---> elapsed time:  44.610000, quantization error: 10.623795\n",
      "\n",
      " Finetune training...\n",
      " radius_ini: 1.000000 , radius_final: 1.000000, trainlen: 1\n",
      "\n",
      " epoch: 1 ---> elapsed time:  44.841000, quantization error: 10.427436\n",
      "\n",
      " Final quantization error: 10.427436\n",
      " train took: 113.828000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSED TIME: 1.93403821786 mins\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "msz0 = 30\n",
    "msz1 = 30\n",
    "\n",
    "sm = SOMPY.sompy.SOM(allWaves, neighborhood=SOMPY.neighborhood.GaussianNeighborhood(), normalizer=SOMPY.normalization.VarianceNormalizator(), mapsize = [msz0, msz1], initialization='pca', name='sm')\n",
    "sm.train(n_job = 1, shared_memory = 'yes')\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'ELAPSED TIME: {} mins'.format(elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codebookVecs = sm.codebook.matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "model = TSNE(n_components=2, method='barnes_hut', verbose=20, n_iter=1000)\n",
    "Y = model.fit_transform(codebookVecs)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'ELAPSED TIME: {} mins'.format(elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "clus = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "cluster_labels = clus.fit_predict(Y)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'ELAPSED TIME: {} mins'.format(elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "uniqueLabels = np.unique(cluster_labels)\n",
    "colors = plt.cm.Paired(np.linspace(0, 1, len(uniqueLabels)))\n",
    "for indLabel, label in enumerate(np.unique(cluster_labels)):\n",
    "    plt.hold(1)\n",
    "    indsThisLabel = np.flatnonzero(cluster_labels==label)\n",
    "    plt.plot(Y[indsThisLabel, 0], Y[indsThisLabel, 1], '.', color=colors[indLabel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad clustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_labels = sm.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "uniqueLabels = np.unique(cluster_labels)\n",
    "colors = plt.cm.Paired(np.linspace(0, 1, len(uniqueLabels)))\n",
    "for indLabel, label in enumerate(np.unique(cluster_labels)):\n",
    "    plt.hold(1)\n",
    "    indsThisLabel = np.flatnonzero(cluster_labels==label)\n",
    "    plt.plot(Y[indsThisLabel, 0], Y[indsThisLabel, 1], '.', color=colors[indLabel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "dataClusters = cluster_labels[sm.project_data(allWaves)]\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'ELAPSED TIME: {} mins'.format(elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "labels = dataClusters\n",
    "rav = allWaves.ravel()\n",
    "maxv = np.percentile(rav, 99.95)\n",
    "minv = np.percentile(rav, 0.05)\n",
    "for indLabel, label in enumerate(uniqueLabels):\n",
    "    plt.subplot(len(uniqueLabels), 1, indLabel+1)\n",
    "    clusterWaves = allWaves[labels==label]\n",
    "\n",
    "    spikesToPlot = np.random.randint(len(clusterWaves),size=50)\n",
    "\n",
    "    for wave in clusterWaves[spikesToPlot]:\n",
    "        plt.plot(wave, '-', alpha=0.5, color=colors[indLabel])\n",
    "        plt.hold(1)\n",
    "    plt.plot(clusterWaves.mean(0), 'k', lw=2, zorder=10)\n",
    "    plt.ylim([minv, maxv])\n",
    "    plt.axvline(x=40, lw=3, color='k')\n",
    "    plt.axvline(x=80, lw=3, color='k')\n",
    "    plt.axvline(x=120, lw=3, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from jaratoolbox import spikesorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSpikes.clusters = dataClusters\n",
    "\n",
    "GAIN = 5000.0\n",
    "SAMPLING_RATE=30000.0\n",
    "dataSpikes.samples = ((dataSpikes.samples - 32768.0) / GAIN) * 1000.0\n",
    "dataSpikes.timestamps = dataSpikes.timestamps/SAMPLING_RATE\n",
    "\n",
    "cr = spikesorting.ClusterReportFromData(dataSpikes,\n",
    "                                        outputDir='/home/nick/Desktop',\n",
    "                                        filename='som_largedata{}{}{}.png'.format(animalName, ephysFn,'Tetrode{}'.format(tetrode) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not too good. We need to figure out a better way to cluster the codebook vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_labels = sklearn.cluster.KMeans(n_clusters=10).fit_predict(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "uniqueLabels = np.unique(cluster_labels)\n",
    "colors = plt.cm.Paired(np.linspace(0, 1, len(uniqueLabels)))\n",
    "for indLabel, label in enumerate(np.unique(cluster_labels)):\n",
    "    plt.hold(1)\n",
    "    indsThisLabel = np.flatnonzero(cluster_labels==label)\n",
    "    plt.plot(Y[indsThisLabel, 0], Y[indsThisLabel, 1], '.', color=colors[indLabel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "dataClusters = cluster_labels[sm.project_data(allWaves)]\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'ELAPSED TIME: {} mins'.format(elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "dataClusters = cluster_labels[sm.project_data_balltree(allWaves)]\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'ELAPSED TIME: {} mins'.format(elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "labels = dataClusters\n",
    "rav = allWaves.ravel()\n",
    "maxv = np.percentile(rav, 99.95)\n",
    "minv = np.percentile(rav, 0.05)\n",
    "for indLabel, label in enumerate(uniqueLabels):\n",
    "    plt.subplot(len(uniqueLabels), 1, indLabel+1)\n",
    "    clusterWaves = allWaves[labels==label]\n",
    "\n",
    "    spikesToPlot = np.random.randint(len(clusterWaves),size=50)\n",
    "\n",
    "    for wave in clusterWaves[spikesToPlot]:\n",
    "        plt.plot(wave, '-', alpha=0.5, color=colors[indLabel])\n",
    "        plt.hold(1)\n",
    "    plt.plot(clusterWaves.mean(0), 'k', lw=2, zorder=10)\n",
    "    plt.ylim([minv, maxv])\n",
    "    plt.axvline(x=40, lw=3, color='k')\n",
    "    plt.axvline(x=80, lw=3, color='k')\n",
    "    plt.axvline(x=120, lw=3, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSpikes.samples #Already in uV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataSpikes.clusters = dataClusters\n",
    "\n",
    "cr = spikesorting.ClusterReportFromData(dataSpikes,\n",
    "                                        outputDir='/home/nick/Desktop',\n",
    "                                        filename='som_largedata_TSNE_KMEANS-{}-{}-{}.png'.format(animalName, ephysFn,'Tetrode{}'.format(tetrode) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach to clustering the codebook vectors\n",
    "\n",
    "This paper:\n",
    "\n",
    "http://publications.mi.fu-berlin.de/133/1/SC-99-38.pdf\n",
    "\n",
    "talks about an approach to clustering the codebook vectors that is automatic and also takes into account the topology of the SOM, which is one of the reasons to use a SOM in the first place. \n",
    "\n",
    "Another one:\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.387.7910&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import SOMPY.hitmap\n",
    "import SOMPY.umatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(codebookVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg = sklearn.cluster.AgglomerativeClustering(n_clusters=12)\n",
    "cluster_labels = agg.fit_predict(codebookVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "uniqueLabels = np.unique(cluster_labels)\n",
    "colors = plt.cm.Paired(np.linspace(0, 1, len(uniqueLabels)))\n",
    "for indLabel, label in enumerate(np.unique(cluster_labels)):\n",
    "    plt.hold(1)\n",
    "    indsThisLabel = np.flatnonzero(cluster_labels==label)\n",
    "    plt.plot(Y[indsThisLabel, 0], Y[indsThisLabel, 1], '.', color=colors[indLabel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSED TIME: 3.30719913244 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "dataClustersSlow = cluster_labels[sm.project_data(allWaves)]\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'ELAPSED TIME: {} mins'.format(elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELAPSED TIME: 1.89144690037 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "dataClusters = cluster_labels[sm.project_data_balltree(allWaves)]\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print 'ELAPSED TIME: {} mins'.format(elapsed/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 8, 5, ..., 9, 1, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataClustersSlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999910548800997"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dataClusters==3)/np.double(len(dataClusters)) # I guess the balltree approx is bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/parallel.py:13: ShimWarning: The `IPython.parallel` package has been deprecated. You should import from ipyparallel instead.\n",
      "  \"You should import from ipyparallel instead.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
